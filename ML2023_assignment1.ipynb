{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "*Part of the course:\n",
    "Machine Learning (code: INFOB3ML), fall 2023, Utrecht University*\n",
    "\n",
    "Total points: 10\n",
    "\n",
    "Deadline: Friday 15 September, 23:59\n",
    "\n",
    "**Write your names and student numbers here: ___**\n",
    "\n",
    "Submit one ipynb file per pair.\n",
    "\n",
    "**Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Regularisation\n",
    "In this assignment, you will perform several simulation experiments with linear regression, in order to investigate the effects of regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Data generation\n",
    "Every datapoint $(x,t)$ will be sampled randomly, with both $x$ and $t$ in $\\mathbb{R}$. (Note that the book from Introduction to Machine Learning called the output $y$, while this course's book calls it $t$.) The input $x$ is uniformly distributed between $-1$ and $1$.\n",
    "\n",
    "Once we have our input $x$, we generate our output $t$ according to $t = 1 - \\cos(x) + \\epsilon$, where $\\epsilon$ is normally distributed with expected value = 0 and variance = $\\sigma^2$.\n",
    "All the random numbers should be generated **independently** from one another.\n",
    "\n",
    "### Regression\n",
    "\n",
    "We'll implement *regularised* regression, adding a penalty $\\lambda \\mathbf{w}^T \\mathbf{w}$ to our training loss. Linear regression with this form of regularisation penalty is also called *ridge regression*. We are going to try out different values of $\\lambda$.\n",
    "\n",
    "We'll use regression with order five polynomials like in the book. This means that for each weight vector $\\mathbf{w}$, our hypothesis is of the form\n",
    "$$f(x; \\mathbf{w}) = \\sum_{i=0}^5 w_i x^i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Code\n",
    "\n",
    "To make it clear what your code is supposed to do and how it should be formatted, we provide you general schema for each to-be-written function. Some functions come with additional hints about useful in-built functions or procedural details. You might write the function's body differently than the hints suggests. That's totally fine as long as the function works as it supposed to work.\n",
    "\n",
    "Use numpy arrays for functionalities involving vectors and matrices. [Here is an overview of numpy, which includes many functions you'll find useful in this course, as well as some more advanced ones.](https://numpy.org/doc/stable/user/quickstart.html)\n",
    "\n",
    "Some specific notes about numpy:\n",
    "\n",
    "* Make sure to use numpy arrays (`np.ndarray`), not `np.matrix`! Arrays are the standard choice, and they can represent vectors, matrices, as well as more general objects. Some of the pre-written code below won't work if you use `np.matrix`.\n",
    "\n",
    "* To multiply two matrices, write `A @ B`. It is a shorthand for the function [`matmul`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html).\n",
    "\n",
    "* To generate random numbers, use for instance [`rg.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.uniform.html#numpy.random.Generator.uniform) and [`rg.normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html#numpy.random.Generator.normal). (The `rg` here is a random number generator object, created in the TEST block below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1** (1 point)\n",
    "\n",
    "Write a function `generate_data` you can use to generate a dataset and outputs the pair of vectors `(x,t)`, accepting parameters $N$ and $\\sigma^2$. Be sure to check if your normal-distribution-generator needs $\\sigma$ (standard deviation) or $\\sigma^2$ (variance) as input parameter. Both `x` and `t` should be 1-dimensional numpy arrays, i.e. their shape should be `(N,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Some functions you may find useful (here and later):\n",
    "# np.cos, np.ones, np.linalg.inv, np.hstack, np.matmul (or @), np.eye, math.sqrt.\n",
    "\n",
    "def generate_data(N, sigma_squared):\n",
    "    # Your code here\n",
    "    x = rg.uniform(-1, 1, N)\n",
    "    t = np.ndarray((N,))\n",
    "    for i in range(N):\n",
    "        eps_i = rg.normal(0, np.sqrt(sigma_squared))\n",
    "        t[i] = 1 - np.cos(x[i])+eps_i\n",
    "    \n",
    "    #print(x)\n",
    "    #print(t)\n",
    "        return x,t\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[-0.3763371  -0.1533471   0.65540519]\n",
      "(3,)\n",
      "[0.25374863 0.66541541 0.38389081]\n"
     ]
    }
   ],
   "source": [
    "# ██████████ TEST ██████████\n",
    "# (These \"TEST\" blocks can help you quickly check if there's something\n",
    "# obviously wrong with the code you wrote.)\n",
    "# Setting a seed with the optional argument to default_rng below helps to\n",
    "# make the data generation deterministic for comparison reasons.\n",
    "rg = np.random.default_rng(seed = 1)\n",
    "toy_xs, toy_t = generate_data(3, 0.1)\n",
    "# Check if the shapes of the output arrays are as specified above:\n",
    "print(toy_xs.shape)\n",
    "print(toy_xs)\n",
    "print(toy_t.shape)\n",
    "print(toy_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to learn 5th-order polynomials, we want to have a function that computes the matrix $\\mathbf{X}$. (See equation (1.18) on page 28 of the book for what this matrix should look like.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** (1 point)\n",
    "\n",
    "(a) When dealing with $N$ data points, what should the size of the $\\mathbf{X}$-matrix be? (Give the number or rows and the number of columns.)\n",
    "\n",
    "(b) Where in the matrix will you find the vector `x_scalars`?\n",
    "\n",
    "(c) Where in the matrix will you find $\\mathbf{x}_1$ (the feature vector associated to the first data point)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) = The size would be N times 6(Heighth is N, width is 6)\n",
    "\n",
    "(b) = The second column of the matrix\n",
    "\n",
    "(C) = The first row of the matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3** (1 point)\n",
    "\n",
    "Write a function `compute_X_matrix` that takes a numpy array `x_scalars` as produced by your code above, and returns the matrix `X` needed by linear regression with 5th-order polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_X_matrix(x_scalars):\n",
    "    # Your code here\n",
    "    X = np.ndarray(shape =(len(x_scalars),6))\n",
    "    for j in range(len(x_scalars)):\n",
    "        X[j,0] = 1\n",
    "        X[j,1] = x_scalars[j]\n",
    "        X[j,2] = x_scalars[j]**2\n",
    "        X[j,3] = x_scalars[j]**3\n",
    "        X[j,4] = x_scalars[j]**4\n",
    "        X[j,5] = x_scalars[j]**5\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3763371  -0.1533471   0.65540519]\n",
      "[[ 1.00000000e+00 -3.76337096e-01  1.41629610e-01 -5.33004761e-02\n",
      "   2.00589464e-02 -7.54892563e-03]\n",
      " [ 1.00000000e+00 -1.53347102e-01  2.35153337e-02 -3.60600828e-03\n",
      "   5.52970919e-04 -8.47964880e-05]\n",
      " [ 1.00000000e+00  6.55405188e-01  4.29555960e-01  2.81533205e-01\n",
      "   1.84518323e-01  1.20934266e-01]]\n"
     ]
    }
   ],
   "source": [
    "# ██████████ TEST ██████████\n",
    "toy_X = compute_X_matrix(toy_xs)\n",
    "print(toy_xs)\n",
    "print(toy_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function provided below will plot the target function $1 - \\cos(x)$, a regression function, and the training data all in one plot. It will be helpful in testing your code for the task below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_regression_result(xs_train, t_train, w, include_target_function=True):\n",
    "    xs_plot = np.linspace(-2, 2, 101)\n",
    "    X_plot = compute_X_matrix(xs_plot)\n",
    "    if include_target_function:\n",
    "        plt.plot(xs_plot, 1 - np.cos(xs_plot), c='black') # target \n",
    "    plt.scatter(xs_train, t_train, c='blue', marker=\".\")\n",
    "    plt.plot(xs_plot, X_plot @ w, c='red')\n",
    "    plt.ylim(-1, 1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4** (1 point)\n",
    "\n",
    "Write code that fits a regularised linear regression hypothesis to training data, in other words, a function that computes our $\\hat{\\mathbf{w}}$. Use numpy to carry out the necessary matrix operations to find an analytic solution; don't use linear regression functionality from Python packages for machine learning. In other words, compute $\\hat{\\mathbf{w}}$ according to the equation (1.21) on page 36 of the book. Give your function a parameter `lamb` which tells it the value of $\\lambda$.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, t, lamb):\n",
    "    # Your code here w = (XTX + NλI)−1XTt\n",
    "    XT = X.transpose\n",
    "    XTX = np.multiply(XT, X)\n",
    "    N = X.shape[0]\n",
    "    I = numpy.identity(N)\n",
    "    NLI = N*lamb*I\n",
    "    Sum = np.add(XTX, NLI)\n",
    "    SUM_invert = np.invert(Sum)\n",
    "    w_hat = np.multiply(Sum_invert, X.transpose,t)\n",
    "    return w_hat\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'builtin_function_or_method' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ██████████ TEST ██████████\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plot_regression_result(toy_xs, toy_t, \u001b[43mfit_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoy_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoy_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m plot_regression_result(toy_xs, toy_t, fit_ridge(toy_X, toy_t, \u001b[38;5;241m100\u001b[39m))\n",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m, in \u001b[0;36mfit_ridge\u001b[1;34m(X, t, lamb)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_ridge\u001b[39m(X, t, lamb):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Your code here w = (XTX + NλI)−1XTt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     XT \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtranspose\n\u001b[1;32m----> 4\u001b[0m     XTX \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     N \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m     I \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39midentity(N)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'builtin_function_or_method' and 'float'"
     ]
    }
   ],
   "source": [
    "# ██████████ TEST ██████████\n",
    "plot_regression_result(toy_xs, toy_t, fit_ridge(toy_X, toy_t, 0.01))\n",
    "plot_regression_result(toy_xs, toy_t, fit_ridge(toy_X, toy_t, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared loss over the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code (which you don't need to change) evaluates a learned linear regression function $\\hat{\\mathbf{w}}$ with respect to the data $\\mathbf{X}, \\mathbf{t}$ using the squared error loss. This Python function can be used to compute training, validation or test loss for $\\hat{\\mathbf{w}}$, depending of the kind of data passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(w, X, t):\n",
    "    N, k = X.shape\n",
    "    t_hat = X @ w\n",
    "    t_error = t_hat - t\n",
    "    sum_of_squared_errors = t_error.T @ t_error\n",
    "    loss = sum_of_squared_errors / N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to do some experiments and look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5** (1 point)\n",
    "\n",
    "Generate a dataset of $N=15$ data points with noise level $\\sigma^2 = 0.1$. (You'll use this dataset in tasks/questions 5 through 7.) For the values of $\\lambda$ provided in the code below, fit a regularised regression curve to the data and compute the loss. Display the results in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = 10 ** np.linspace(-5, 5, 11)\n",
    "losses = np.zeros_like(lambdas)\n",
    "\n",
    "rg = np.random.default_rng(seed = 1)\n",
    "\n",
    "# Your code here\n",
    "    \n",
    "plt.semilogx(lambdas, losses, c='black') # target \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6** (1 point) Your plot should show that as $\\lambda$ gets larger, the loss also gets larger. Explain why this is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task/Question 7** (1 point)\n",
    "\n",
    "Using the same data, answer the following questions:\n",
    "\n",
    "For what values of $\\lambda > 0$ do you clearly see overfitting? For what values of $\\lambda$ do you see underfitting? To support your answer, include plots for some values of $\\lambda$, and point out what features of those plots tell you that over-/underfitting is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a good value of $\\lambda$, a variety of techniques exist. One that obviously does *not* work is to look at the training loss as a function of $\\lambda$ (like you plotted above): that would always suggest to make $\\lambda$ as small as possible! A versatile technique that you've already seen in an earlier course (or in section 1.5 of the book) is **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8** (1 point)\n",
    "\n",
    "Write some code to do the following: sample a new dataset of $N = 50$ data points and $\\sigma^2=0.1$. (You'll use this dataset for all the remaining tasks and questions.) Write a function that, given data and value of $\\lambda$, computes the leave-one-out cross-validation (LOOCV) loss, as explained in section 1.5.2 of the book. Then make a plot similar to what we did in task 5 for the training loss, but this time displaying the LOOCV loss as a function of $\\lambda$.\n",
    "\n",
    "Note that the third argument, `fitting_function`, should be the name of a function that `LOOCV` can call to compute `w`. If `fit_ridge` is passed, your previously written function will be used. But later, you'll call it with a different fitting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOOCV(X, t, fitting_function, lamb):\n",
    "    N, k = X.shape\n",
    "    sum_of_losses = 0.0\n",
    "    for leave_out in range(N):\n",
    "        # Your code here\n",
    "        pass\n",
    "        \n",
    "    return sum_of_losses / N\n",
    "\n",
    "# Your code here to sample a larger dataset, and to make the plot of the LOOCV loss for each lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9** (0.5 points): What value of $\\lambda$ does LOOCV point you to? Look at a plot of the resulting regression function. Does it look reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the book mentions, when doing regularisation, using the squares of $\\mathbf{w}$ as a penalty is just one of many possibilities. It has the advantage of having an analytical solution. But other options exist that may have other advantages, and while they may not be analytically computable, still there exist efficient algorithms for working with them. A particularly popular one is to use the sum of absolute values of $\\mathbf{w}$ as a penalty: we will find the $\\mathbf{w}$ that minimizes\n",
    "$$\\mathcal{L}' = \\mathcal{L} + \\lambda \\sum_i \\lvert w_i \\rvert.$$\n",
    "This is called the 'lasso' (which is an acronym for 'least absolute shrinkage and selection operator', but of course most people just remember the acronym)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no direct formula for computing the $\\mathbf{w}$ that minimizes $\\mathcal{L}'$. The next alternative would be to use (stochastic) gradient descent. Unfortunately, that also doesn't work very nicely here, because as a function of $\\mathbf{w}$, $\\mathcal{L}'$ is not differentiable wherever $\\mathbf{w}$ has at least one entry equal to zero. But variants of gradient descent have been developed that can deal with this problem (such as [proximal gradient descent](https://en.wikipedia.org/wiki/Proximal_gradient_method)), and implementations are readily available. The fitting function provided below uses such an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def fit_lasso(X, t, lamb):\n",
    "    clf = Lasso(lamb, fit_intercept=False, max_iter=100000)\n",
    "    clf.fit(X, t)\n",
    "    return clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10** (0.5 point) Again plot the LOOCV losses as a function of $\\lambda$, but this time for lasso regression instead of ridge regression. Read off the values of $\\lambda$ that minimizes the LOOCV loss, and display the regression function for that $\\lambda$ in a separate plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of lasso regularisation is its tendency to make some weigths exactly equal to 0. (Well, mathematically that's true, but you should never rely on things being *exactly* equal when a numerical algorithm is involved. Instead, check whether the difference between them is very small, say less than `1e-9`.)\n",
    "\n",
    "**Task 11** (0.5 points)\n",
    "\n",
    "What is the smallest $\\lambda$ in `lambdas` for which you observe this happening for some $w_i$? For that combination of $\\lambda$ and $i$, make a plot where $w_i$ varies along the horizontal axis. On the vertical axis, plot the regularised loss $\\mathcal{L}'$ of the weight vector, with all entries other than $w_i$ kept equal to the optimal lasso solution. Choose the range of $w_i$-values small enough that you see a nondifferentiability in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12** (0.5 points): Use this graph to explain why lasso regression has a tendency to make some weights equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember: Before you submit, click Kernel > Restart & Run All to make sure you submit a working version of your code!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
